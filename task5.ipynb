{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 了解Dropout, L1, L2正则化的原理并用代码实现\n",
    "# Dropout的numpy实现\n",
    "# Dropout的pytorch实现\n",
    "\n",
    "'''训练神经网络的时候经常会遇到过拟合的问题，过拟合具体表现在：模型在训练数据上损失函数较小\n",
    "预测准确率较高；但是在测试数据上损失函数比较大。为了解决这种问题Dropout就出现了，我们在前向\n",
    "传播的时候，让某个神经元的激活值以一定的概率p停止工作，这样可以使模型泛化性更强，因为它不会太依赖某些局部的特征，'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.autograd import Variable\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L1 正则化\n",
    "regularization_loss = 0\n",
    "for param in model.parameters():\n",
    "    regularization_loss += torch.sum(abs(param))\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    y_pred = model(x_train)\n",
    "    classify_loss = criterion(y_pred, y_train.float().view(-1, 1))\n",
    "    loss = classify_loss + 0.001 * regularization_loss  # 引入L1正则化项"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实现L2范数\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-1, momentum=0.9, weight_decay=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#在numpy中实现dropout\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])\n",
    "\n",
    "y = np.array([[0,1,1,0]]).T\n",
    "\n",
    "alpha,hidden_dim,dropout_percent,do_dropout = (0.5,4,0.2,True)\n",
    "\n",
    "synapse_0 = 2*np.random.random((3,hidden_dim)) - 1\n",
    "\n",
    "synapse_1 = 2*np.random.random((hidden_dim,1)) - 1\n",
    "\n",
    "for j in xrange(60000):\n",
    "\n",
    "    layer_1 = (1/(1+np.exp(-(np.dot(X,synapse_0)))))\n",
    "    if(do_dropout):\n",
    "        layer_1 *= np.random.binomial([np.ones((len(X),hidden_dim))],1-dropout_percent)[0] * (1.0/(1-dropout_percent))\n",
    "    layer_2 = 1/(1+np.exp(-(np.dot(layer_1,synapse_1))))\n",
    "    layer_2_delta = (layer_2 - y)*(layer_2*(1-layer_2))\n",
    "    layer_1_delta = layer_2_delta.dot(synapse_1.T) * (layer_1 * (1-layer_1))\n",
    "    synapse_1 -= (alpha * layer_1.T.dot(layer_2_delta))\n",
    "    synapse_0 -= (alpha * X.T.dot(layer_1_delta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch中实现dropout\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# torch.manual_seed(1)    # reproducible\n",
    "\n",
    "N_SAMPLES = 20\n",
    "N_HIDDEN = 300\n",
    "\n",
    "# training data\n",
    "x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "y = x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "x, y = Variable(x), Variable(y)\n",
    "\n",
    "# test data\n",
    "test_x = torch.unsqueeze(torch.linspace(-1, 1, N_SAMPLES), 1)\n",
    "test_y = test_x + 0.3*torch.normal(torch.zeros(N_SAMPLES, 1), torch.ones(N_SAMPLES, 1))\n",
    "test_x, test_y = Variable(test_x, volatile=True), Variable(test_y, volatile=True)\n",
    "\n",
    "# show data\n",
    "'''\n",
    "plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.5, label='train')\n",
    "plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.5, label='test')\n",
    "plt.legend(loc='upper left')\n",
    "plt.ylim((-2.5, 2.5))\n",
    "plt.show()\n",
    "'''\n",
    "\n",
    "net_overfitting = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "\n",
    "net_dropped = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "    torch.nn.Dropout(0.5),  # drop 50% of the neuron\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(N_HIDDEN, 1),\n",
    ")\n",
    "\n",
    "print(net_overfitting)  # net architecture\n",
    "print(net_dropped)\n",
    "\n",
    "optimizer_ofit = torch.optim.Adam(net_overfitting.parameters(), lr=0.01)\n",
    "optimizer_drop = torch.optim.Adam(net_dropped.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "\n",
    "plt.ion()   # something about plotting\n",
    "\n",
    "for t in range(500):\n",
    "    pred_ofit = net_overfitting(x)\n",
    "    pred_drop = net_dropped(x)\n",
    "\n",
    "    loss_ofit = loss_func(pred_ofit, y)\n",
    "    loss_drop = loss_func(pred_drop, y)\n",
    "\n",
    "    optimizer_ofit.zero_grad()\n",
    "    optimizer_drop.zero_grad()\n",
    "    loss_ofit.backward()\n",
    "    loss_drop.backward()\n",
    "    optimizer_ofit.step()\n",
    "    optimizer_drop.step()\n",
    "\n",
    "    if t % 10 == 0:\n",
    "        # change to eval mode in order to fix drop out effect\n",
    "        net_overfitting.eval()\n",
    "        net_dropped.eval()  # parameters for dropout differ from train mode\n",
    "\n",
    "        # plotting\n",
    "        plt.cla()\n",
    "        test_pred_ofit = net_overfitting(test_x)\n",
    "        test_pred_drop = net_dropped(test_x)\n",
    "        plt.scatter(x.data.numpy(), y.data.numpy(), c='magenta', s=50, alpha=0.3, label='train')\n",
    "        plt.scatter(test_x.data.numpy(), test_y.data.numpy(), c='cyan', s=50, alpha=0.3, label='test')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_ofit.data.numpy(), 'r-', lw=3, label='overfitting')\n",
    "        plt.plot(test_x.data.numpy(), test_pred_drop.data.numpy(), 'b--', lw=3, label='dropout(50%)')\n",
    "        plt.text(0, -1.2, 'overfitting loss=%.4f' % loss_func(test_pred_ofit, test_y).data[0], fontdict={'size': 20, 'color':  'red'})\n",
    "        plt.text(0, -1.5, 'dropout loss=%.4f' % loss_func(test_pred_drop, test_y).data[0], fontdict={'size': 20, 'color': 'blue'})\n",
    "        plt.legend(loc='upper left'); plt.ylim((-2.5, 2.5));plt.pause(0.1)\n",
    "\n",
    "        # change back to train mode\n",
    "        net_overfitting.train()\n",
    "        net_dropped.train()\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
